{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io, os, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "# initializations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu, torch.cuda.get_device_name(0))\n",
    "MAX_LEN = 128\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, it is recommend a batch size of 16 or 32\n",
    "batch_size = 128\n",
    "# Number of training epochs (it is recommend between 2 and 4 - we dont want to overtrain, just fine tune the model)\n",
    "epochs = 8\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataPreprocessorLoader:\n",
    "    def __init__(self, dataframe_path):\n",
    "        tokenizer_path = '../bertPytorch/bert-base-cased'\n",
    "        assert os.path.exists(tokenizer_path)\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "               \n",
    "        df = pd.read_csv(dataframe_path, usecols=['text', 'truth', 'split'])\n",
    "        train_df = df[df['split']=='TRAIN']\n",
    "        test_df = df[df['split']=='TEST']\n",
    "        train_labels = train_df.truth.values\n",
    "        test_labels = test_df.truth.values\n",
    "        \n",
    "        train_valid_inputs, train_valid_masks = self.preprocess_for_BERT(train_df, tokenizer)       \n",
    "        train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_valid_inputs, train_labels, random_state=2018, test_size=0.1)\n",
    "        train_masks, validation_masks, _, _ = train_test_split(train_valid_masks, train_valid_inputs, random_state=2018, test_size=0.1)\n",
    "        print(train_inputs.shape, validation_inputs.shape, train_labels.shape, validation_labels.shape)\n",
    "        \n",
    "        test_inputs, test_masks = self.preprocess_for_BERT(test_df, tokenizer)\n",
    "        print(test_inputs.shape, test_labels.shape)\n",
    "        \n",
    "        train_inputs = torch.tensor(train_inputs)\n",
    "        validation_inputs = torch.tensor(validation_inputs)\n",
    "        test_inputs = torch.tensor(test_inputs)\n",
    "        train_labels = torch.tensor(train_labels)\n",
    "        validation_labels = torch.tensor(validation_labels)\n",
    "        test_labels = torch.tensor(test_labels)\n",
    "        train_masks = torch.tensor(train_masks)\n",
    "        validation_masks = torch.tensor(validation_masks)\n",
    "        test_masks = torch.tensor(test_masks)\n",
    "        \n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        self.train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "        \n",
    "        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        self.validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "        \n",
    "        test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "        test_sampler = RandomSampler(test_data)\n",
    "        self.test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "            \n",
    "    def preprocess_for_BERT(self, df, tokenizer):\n",
    "        sentences = df['text'].values\n",
    "        sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "        tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        \n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return input_ids, attention_masks        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertVictimModel:\n",
    "    def __init__(self):\n",
    "        pretrained_model_path = '../bertPytorch/bert-base-cased'\n",
    "        assert os.path.exists(pretrained_model_path)\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(pretrained_model_path, num_labels=num_labels)\n",
    "        self.model.cuda()\n",
    "        \n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)\n",
    "        \n",
    "    def flat_accuracy(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.model, \"bert_model_128_128_8.pth\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.model = torch.load(\"bert_model_128_128_8.pth\")\n",
    "        \n",
    "    def fit(self, train_dataloader, validation_dataloader):\n",
    "        train_loss_set = [] # Store our loss and accuracy for plotting\n",
    "        for _ in trange(epochs, desc=\"Epoch\"): # trange is a tqdm wrapper around the normal python range\n",
    "            ########### training\n",
    "            self.model.train() # Set our model to training mode (as opposed to evaluation mode)\n",
    "            tr_loss, nb_tr_examples, nb_tr_steps = 0, 0, 0  # Tracking variables\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader): # Train the data for one epoch\n",
    "                print('#', end='')\n",
    "                b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch) # Add batch to GPU and Unpack the inputs from our dataloader\n",
    "    \n",
    "                self.optimizer.zero_grad() # Clear out the gradients (by default they accumulate)\n",
    "                loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Forward pass\n",
    "                train_loss_set.append(loss.item())    \n",
    "                loss.backward() # Backward pass\n",
    "                self.optimizer.step() # Update parameters and take a step using the computed gradient\n",
    "                \n",
    "                # Update tracking variables\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += b_input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                \n",
    "                if step%100==0:\n",
    "                    print(step)\n",
    "            \n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "            \n",
    "            ########### Validation\n",
    "            self.model.eval() # Put model in evaluation mode to evaluate loss on the validation set\n",
    "            eval_loss, eval_accuracy, nb_eval_steps, nb_eval_examples = 0, 0, 0, 0 # Tracking variables \n",
    "\n",
    "            for batch in validation_dataloader: # Evaluate data for one epoch\n",
    "                # Add batch to GPU and Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "                \n",
    "                with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                    logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Forward pass, calculate logit predictions\n",
    "\n",
    "                # Move logits and labels to CPU and compute accuracy\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                eval_accuracy += self.flat_accuracy(logits, label_ids)\n",
    "                \n",
    "                nb_eval_steps += 1 # update tracking var\n",
    "                \n",
    "            print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "            \n",
    "    def predict(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "        predictions , true_labels = [], []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            # Add batch to GPU and Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "                logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Forward pass, calculate logit predictions\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "            print('#', end='')\n",
    "        \n",
    "        print()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i in range(len(true_labels)):\n",
    "            y_true.extend(true_labels[i])\n",
    "            y_pred.extend(np.argmax(predictions[i], axis=1).flatten())\n",
    "        assert len(y_true) == len(y_pred)\n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def evaluate(self, test_dataloader, metric=accuracy_score):\n",
    "        y_true, y_pred = self.predict(test_dataloader)\n",
    "        score = metric(y_true, y_pred)\n",
    "        print(metric.__name__, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victim model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29926, 128) (3326, 128) (29926,) (3326,)\n",
      "(7126, 128) (7126,)\n"
     ]
    }
   ],
   "source": [
    "data_loader = BertDataPreprocessorLoader('combined_relevant_data.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = BertVictimModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.6148226508217999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▎        | 1/8 [04:03<28:23, 243.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.730325663919414\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.41186761703246677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 2/8 [08:06<24:19, 243.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.857195322039072\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.19206706198871645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|███▊      | 3/8 [12:09<20:16, 243.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9007841117216118\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.0854004290408622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 4/8 [16:12<16:12, 243.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9077142475579976\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.04681918455653975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 5/8 [20:16<12:09, 243.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9143057463369964\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.03472864090337649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 6/8 [24:19<08:06, 243.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9113057081807081\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.025075112296363864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  88%|████████▊ | 7/8 [28:22<04:03, 243.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9227192078754579\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "#################################Train loss: 0.021556579534752436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 8/8 [32:25<00:00, 243.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9163995726495726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(data_loader.train_dataloader, data_loader.validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victim model performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = model.predict(data_loader.test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.921274207128824\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victim model performance on test data with adversarial samples mixed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29926, 128) (3326, 128) (29926,) (3326,)\n",
      "(20904, 128) (20904,)\n",
      "####################################################################################################################################################################\n",
      "0.8474454649827784\n"
     ]
    }
   ],
   "source": [
    "data_loader = BertDataPreprocessorLoader('combined_original_adv_data.csv')  \n",
    "model.load_model()\n",
    "y_true, y_pred = model.predict(data_loader.test_dataloader)\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36126, 128) (4015, 128) (36126,) (4015,)\n",
      "(14015, 128) (14015,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.5527713370407428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▎        | 1/8 [04:53<34:14, 293.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8068016539228724\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.2557343966179517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 2/8 [09:47<29:21, 293.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9108471160239362\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.09163843884458601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|███▊      | 3/8 [14:40<24:27, 293.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9252514128989362\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.04726416080722803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 4/8 [19:34<19:34, 293.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9358585438829787\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.029304426568214758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 5/8 [24:28<14:40, 293.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9317756815159575\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.021784698037766316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 6/8 [29:21<09:47, 293.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9371467752659575\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.019415330920078834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  88%|████████▊ | 7/8 [34:15<04:53, 293.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9351936502659575\n",
      "#0\n",
      "####################################################################################################100\n",
      "####################################################################################################200\n",
      "##################################################################################Train loss: 0.015548673731747146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 8/8 [39:08<00:00, 293.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.94140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = BertDataPreprocessorLoader('combined_adv_training_data.csv')  \n",
    "model = BertVictimModel()\n",
    "model.fit(data_loader.train_dataloader, data_loader.validation_dataloader)\n",
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victim model performance on test data with adversarial samples mixed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36126, 128) (4015, 128) (36126,) (4015,)\n",
      "(14015, 128) (14015,)\n",
      "##############################################################################################################\n",
      "0.9242240456653585\n"
     ]
    }
   ],
   "source": [
    "data_loader = BertDataPreprocessorLoader('combined_adv_training_data.csv')  \n",
    "model.load_model()\n",
    "y_true, y_pred = model.predict(data_loader.test_dataloader)\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
